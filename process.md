# AI Engine - Complete Architecture Documentation

## ğŸ“‹ Table of Contents

1. [High-Level Architecture](#high-level-architecture)
2. [Technology Stack](#technology-stack)
3. [Core Components](#core-components)
4. [Communication Protocol](#communication-protocol)
5. [Process Lifecycle](#process-lifecycle)
6. [Start Process Flow](#start-process-flow)
7. [Idle Timeout Mechanism](#idle-timeout-mechanism)
8. [Input Handling Flow](#input-handling-flow)
9. [Stop/Shutdown Flow](#stopshutdown-flow)
10. [Unix Socket Communication](#unix-socket-communication)
11. [Framework Comparison](#framework-comparison)

---

## High-Level Architecture

The application uses a **three-tier distributed architecture** with Unix Domain Socket IPC for enterprise-grade performance:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TAURI APPLICATION                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     FRONTEND TIER (React/TypeScript)                     â”‚   â”‚
â”‚  â”‚     â”œâ”€ App.tsx (UI Components)                           â”‚   â”‚
â”‚  â”‚     â”œâ”€ State Management (useState)                       â”‚   â”‚
â”‚  â”‚     â”œâ”€ Event Listeners (python_status, python_input)     â”‚   â”‚
â”‚  â”‚     â””â”€ Command Invocation                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚ Tauri invoke()                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     MIDDLEWARE TIER (Rust/Tauri)                         â”‚   â”‚
â”‚  â”‚     â”œâ”€ lib.rs (Process Manager)                          â”‚   â”‚
â”‚  â”‚     â”œâ”€ socket_http_get() (Unix Socket GET)               â”‚   â”‚
â”‚  â”‚     â”œâ”€ socket_http_post() (Unix Socket POST)             â”‚   â”‚
â”‚  â”‚     â”œâ”€ Activity Tracking (idle timeout)                  â”‚   â”‚
â”‚  â”‚     â”œâ”€ Process Lifecycle Management                      â”‚   â”‚
â”‚  â”‚     â””â”€ Event Emission System                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚ Unix Domain Socket (IPC)                â”‚
â”‚                       â”‚ /tmp/ai-engine.sock                     â”‚
â”‚                       â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     APPLICATION TIER (Python/Hypercorn)                  â”‚   â”‚
â”‚  â”‚     â”œâ”€ Binary (PyInstaller compiled executable)          â”‚   â”‚
â”‚  â”‚     â”œâ”€ Hypercorn ASGI Server                             â”‚   â”‚
â”‚  â”‚     â”œâ”€ Starlette Framework                               â”‚   â”‚
â”‚  â”‚     â”œâ”€ Route Handlers                                    â”‚   â”‚
â”‚  â”‚     â”‚  â”œâ”€ GET /status (health polling)                   â”‚   â”‚
â”‚  â”‚     â”‚  â”œâ”€ POST /input (user requests)                    â”‚   â”‚
â”‚  â”‚     â”‚  â”œâ”€ GET /health (startup verification)             â”‚   â”‚
â”‚  â”‚     â”‚  â””â”€ POST /stop (graceful shutdown)                 â”‚   â”‚
â”‚  â”‚     â”œâ”€ AppState (memory-resident ML models)              â”‚   â”‚
â”‚  â”‚     â””â”€ Background Processing                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Characteristics

- **Decoupled Tiers**: Frontend, middleware, and application run independently
- **Unix Socket IPC**: Direct kernel communication, no TCP overhead
- **Memory Optimization**: Python keeps ML models resident (5-min idle timeout)
- **Binary Distribution**: PyInstaller compiled, platform-specific binaries
- **Event-Driven**: Frontend reactive updates via Tauri events
- **Graceful Degradation**: Clean shutdown with 500ms grace period

---

## Technology Stack

| Layer | Framework | Language | Why This Choice |
|-------|-----------|----------|-----------------|
| **Frontend** | React + Vite | TypeScript | Fast dev server, hot reload, modern tooling |
| **IPC** | Tauri Commands | Rust â†” TS | Type-safe, minimal overhead, native integration |
| **Middleware** | Tauri Plugin Shell | Rust | Process spawning, child management, cross-platform |
| **Socket IPC** | Tokio UnixStream | Rust | Async I/O, non-blocking socket communication |
| **Backend Server** | Hypercorn | Python | ASGI server with native Unix socket support |
| **Web Framework** | Starlette | Python | Lightweight, async-first, perfect for sidecars |
| **Binary Build** | PyInstaller | Python | Self-contained executable, no runtime dependency |

---

## Core Components

### 1. Frontend (`src/App.tsx`)

**Responsibilities**:
- Display UI (Start/Stop buttons, input form, output cards)
- Invoke Rust commands via `invoke()` API
- Listen for events from Rust backend
- Update UI based on received data

**Key Functions**:
- `startPython()`: Triggers backend startup, sets up event listeners
- `stopPython()`: Graceful shutdown of backend
- `sendInput()`: Sends user input to backend

### 2. Middleware (`src-tauri/src/lib.rs`)

**Responsibilities**:
- Spawn/manage Python process
- Communicate with Python via Unix socket
- Track idle state and enforce timeouts
- Emit events to frontend
- Graceful process shutdown

**Key Functions**:
- `start_python_script()`: Spawn binary, wait for socket, start polling
- `socket_http_get()`: Send HTTP GET over Unix socket
- `socket_http_post()`: Send HTTP POST over Unix socket
- `stop_python_script()`: Graceful shutdown
- `send_input_to_python()`: Route user input
- `on_app_interaction()`: Reset idle timer

### 3. Application (`python/app.py`)

**Responsibilities**:
- Listen on Unix socket at `/tmp/ai-engine.sock`
- Handle HTTP requests over socket
- Process application logic
- Return JSON responses
- Graceful shutdown handling

**Key Endpoints**:
- `GET /status`: Return current state (called every 1 sec)
- `POST /input`: Process user input
- `GET /health`: Health check (startup verification)
- `POST /stop`: Graceful shutdown signal

---

## Communication Protocol

### Unix Domain Socket + HTTP/1.1

```
Rust â†’ Unix Socket (/tmp/ai-engine.sock) â†’ Hypercorn â†’ Starlette â†’ Python Logic
  â†“                                                                      â†“
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Response (JSON) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Protocol Details**:
- **Transport**: Unix Domain Socket (AF_UNIX)
- **HTTP Version**: HTTP/1.1 (not HTTP/2, for simplicity)
- **Content-Type**: application/json
- **Connection**: close (single-request connections)
- **Format**: Standard HTTP request/response format

**Example GET Request**:
```
GET /status HTTP/1.1\r\n
Host: localhost\r\n
Connection: close\r\n
\r\n
```

**Example POST Request**:
```
POST /input HTTP/1.1\r\n
Host: localhost\r\n
Content-Type: application/json\r\n
Content-Length: 24\r\n
Connection: close\r\n
\r\n
{"input": "user text"}
```

---

## Process Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROCESS LIFECYCLE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] APP START
    â†“
    User opens application
    Frontend loads, Tauri initializes
    No backend process spawned yet

[2] START PHASE (User clicks "Start Python Script")
    â†“
    Frontend: invoke("start_python_script")
    â†“
    Rust: Spawn binary ../src-tauri/binaries/ai-engine-aarch64-apple-darwin
    â†“
    Python: Initialize Hypercorn on /tmp/ai-engine.sock
    â†“
    Rust: Wait for socket file to exist (max 20 attempts, 500ms intervals)
    â†“
    Python: Socket created, ready for connections
    â†“
    Rust: Start polling loop (1 second intervals) â†’ polling runs forever
    â†“
    Frontend: Receive updates via "python_status" events

[3] IDLE MONITORING (Continuous background task)
    â†“
    Every 1 second:
      Check: (current_time - last_activity_time) > 300 seconds?
      
      If NO  â†’ Continue polling, emit status event
      If YES â†’ Enter STOP PHASE

[4] USER INPUT (Optional, can repeat multiple times)
    â†“
    User types text and clicks "Send Input"
    â†“
    Frontend: invoke("send_input_to_python", { input })
    â†“
    Rust: Update last_activity = now() (resets idle timer)
    â†“
    Rust: socket_http_post("/input", { input })
    â†“
    Python: Process, return response
    â†“
    Rust: Emit "python_input" event
    â†“
    Frontend: Update display with response

[5] STOP PHASE (Triggered by: idle timeout OR user clicks "Stop")
    â†“
    Rust: socket_http_post("/stop", {})
    â†“
    Python: Graceful shutdown (CloseConnection signal)
    â†“
    Rust: Wait 500ms for graceful shutdown
    â†“
    Rust: Force kill process if still alive
    â†“
    Rust: Mark is_running = false
    â†“
    Rust: Polling loop exits
    â†“
    Frontend: Receive stop notification
    â†“
    User can click "Start" again to restart

[6] APP SHUTDOWN
    â†“
    User closes application
    â†“
    If process running â†’ Tauri kills it automatically
```

---

## Start Process Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     START_PYTHON_SCRIPT() - DETAILED FLOW          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRIGGER: User clicks "Start Python Script" button
         Frontend calls: invoke("start_python_script")

STEP 1: Check if already running
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ proc_state = lock(PythonProcess) â”‚
  â”‚ if is_running == true:           â”‚
  â”‚   return Ok() â† Already running  â”‚
  â”‚ else:                            â”‚
  â”‚   continue                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: Determine binary path (platform-aware)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ #[cfg(target_os = "macos")]              â”‚
  â”‚ #[cfg(target_arch = "aarch64")]          â”‚
  â”‚ binary = "../src-tauri/binaries/         â”‚
  â”‚           ai-engine-aarch64-apple-darwin"â”‚
  â”‚                                          â”‚
  â”‚ #[cfg(target_arch = "x86_64")]           â”‚
  â”‚ binary = "../src-tauri/binaries/         â”‚
  â”‚           ai-engine-x86_64-apple-darwin" â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: Spawn process via Tauri Shell Plugin
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ child = app.shell()                  â”‚
  â”‚           .command(binary_path)      â”‚
  â”‚           .spawn()                   â”‚
  â”‚                                      â”‚
  â”‚ Result: Background process running  â”‚
  â”‚ Python initializing Hypercorn server â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 4: Store process handle
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PythonProcess {                  â”‚
  â”‚   child: Some(process_handle),   â”‚
  â”‚   last_activity: now(),          â”‚
  â”‚   is_running: false              â”‚
  â”‚ }                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 5: Wait for Unix socket to be ready
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ LOOP from attempt 1 to 20:               â”‚
  â”‚   if Path::exists("/tmp/ai-engine.sock")?â”‚
  â”‚     println!("Socket ready!")            â”‚
  â”‚     break                                â”‚
  â”‚   else:                                  â”‚
  â”‚     tokio::time::sleep(500ms)            â”‚
  â”‚     continue                             â”‚
  â”‚                                          â”‚
  â”‚ if NOT found after 20 attempts:          â”‚
  â”‚   return Err("Socket startup timeout")   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 6: Mark as running
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PythonProcess {                  â”‚
  â”‚   child: Some(...),              â”‚
  â”‚   last_activity: now(),          â”‚
  â”‚   is_running: true â† Updated     â”‚
  â”‚ }                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 7: Spawn async polling loop (background task)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ tauri::async_runtime::spawn(async move {       â”‚
  â”‚   LOOP infinitely:                             â”‚
  â”‚     [Check idle timeout]                       â”‚
  â”‚       elapsed = now() - last_activity          â”‚
  â”‚       if elapsed > 300 seconds:                â”‚
  â”‚         socket_http_post("/stop", {})          â”‚
  â”‚         is_running = false                     â”‚
  â”‚         break loop                             â”‚
  â”‚                                                â”‚
  â”‚     [Poll status]                              â”‚
  â”‚       tokio::time::sleep(1 second)             â”‚
  â”‚       response = socket_http_get("/status")    â”‚
  â”‚       if Ok(json_data):                        â”‚
  â”‚         app.emit("python_status", json_data)   â”‚
  â”‚         â†’ Frontend receives event & updates    â”‚
  â”‚ })                                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RESULT: 
  âœ“ Process spawned and running
  âœ“ Socket file created
  âœ“ Polling loop active (1 sec intervals)
  âœ“ Frontend receives "python_status" events
  âœ“ Idle timeout monitoring enabled
```

---

## Idle Timeout Mechanism

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      IDLE TIMEOUT - MEMORY OPTIMIZATION             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE: 
  Automatically stop Python process after 5 minutes of inactivity
  Save memory from loaded ML models (TensorFlow, PyTorch)
  Prevent resource waste from forgotten instances

CONFIGURATION:
  const IDLE_TIMEOUT_SECS: u64 = 300;  // 5 minutes
  const STATUS_POLL_INTERVAL_SECS: u64 = 1;  // Check every 1 second

ACTIVITY TRACKING:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Events that UPDATE last_activity timestamp:     â”‚
  â”‚                                                  â”‚
  â”‚ 1. start_python_script()                        â”‚
  â”‚    â””â”€ When user clicks "Start"                  â”‚
  â”‚                                                  â”‚
  â”‚ 2. send_input_to_python()                       â”‚
  â”‚    â””â”€ When user clicks "Send Input"             â”‚
  â”‚                                                  â”‚
  â”‚ 3. on_app_interaction()                         â”‚
  â”‚    â””â”€ Optional: Any user interaction            â”‚
  â”‚                                                  â”‚
  â”‚ Events that DON'T reset timer:                  â”‚
  â”‚ - Automatic polling (GET /status)               â”‚
  â”‚ - Status updates from server                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

POLLING LOOP LOGIC:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Runs in background every 1 second:                 â”‚
  â”‚                                                     â”‚
  â”‚ EACH ITERATION:                                    â”‚
  â”‚   1. last_activity_lock = state.last_activity      â”‚
  â”‚   2. elapsed = current_time - last_activity        â”‚
  â”‚   3. unlock                                        â”‚
  â”‚                                                     â”‚
  â”‚   4. IF elapsed > 300 seconds (5 min):             â”‚
  â”‚        println!("Idle timeout reached!")           â”‚
  â”‚        socket_http_post("/stop", {})               â”‚
  â”‚        â”‚                                           â”‚
  â”‚        â”œâ”€ Python receives SIGTERM equivalent      â”‚
  â”‚        â”œâ”€ Python shuts down gracefully            â”‚
  â”‚        â””â”€ Hypercorn closes socket                 â”‚
  â”‚                                                     â”‚
  â”‚        is_running = false                          â”‚
  â”‚        EXIT LOOP â† Stop monitoring                 â”‚
  â”‚                                                     â”‚
  â”‚   5. ELSE (NOT idle yet):                          â”‚
  â”‚        sleep(1 second)                            â”‚
  â”‚        socket_http_get("/status")                 â”‚
  â”‚        emit("python_status", response)            â”‚
  â”‚        CONTINUE LOOP                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EXAMPLE TIMELINE:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ 14:00:00 - User clicks "Start"                       â”‚
  â”‚            last_activity = 14:00:00                  â”‚
  â”‚                                                       â”‚
  â”‚ 14:00:01 - Polling loop 1: elapsed = 1 sec âœ“ OK      â”‚
  â”‚            Continue polling, emit status             â”‚
  â”‚                                                       â”‚
  â”‚ 14:02:15 - User sends input "hello"                  â”‚
  â”‚            last_activity = 14:02:15 â† RESET          â”‚
  â”‚                                                       â”‚
  â”‚ 14:02:16 - Polling loop N: elapsed = 1 sec âœ“ OK      â”‚
  â”‚            Timer effectively "restarted"             â”‚
  â”‚                                                       â”‚
  â”‚ 14:07:15 - User does nothing for 5 minutes           â”‚
  â”‚            last_activity still = 14:02:15            â”‚
  â”‚            elapsed = 14:07:15 - 14:02:15 = 5 min     â”‚
  â”‚                                                       â”‚
  â”‚ 14:07:16 - Polling loop checks: elapsed = 301 sec    â”‚
  â”‚            TIMEOUT REACHED!                          â”‚
  â”‚                                                       â”‚
  â”‚ 14:07:16 - Rust sends /stop to Python                â”‚
  â”‚            Python gracefully shuts down              â”‚
  â”‚            Socket closed, process terminated         â”‚
  â”‚                                                       â”‚
  â”‚ 14:07:17 - Frontend notified (if listening)          â”‚
  â”‚            User can click "Start" to restart          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

MEMORY IMPACT:
  Process alive (5 min): TensorFlow + PyTorch in RAM
  Process stopped: ~95% RAM saved
  Restart time: <1 second (Python startup only)
```

---

## Input Handling Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SEND_INPUT_TO_PYTHON() - DETAILED FLOW          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRIGGER: User types text and clicks "Send Input" button
         Frontend calls: invoke("send_input_to_python", { input: "hello" })

STEP 1: Validate and update idle timer
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ proc_state = lock(PythonProcess)            â”‚
  â”‚ last_activity_lock = lock(last_activity)    â”‚
  â”‚ *last_activity = current_time()             â”‚
  â”‚ // Timer RESET - 5 min countdown starts now â”‚
  â”‚ unlock(last_activity)                       â”‚
  â”‚ unlock(proc_state)                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: Construct HTTP POST request
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ endpoint = "/input"                         â”‚
  â”‚ body = { "input": "hello" }                 â”‚
  â”‚ body_json = serde_json::to_string(body)     â”‚
  â”‚           = "{\"input\": \"hello\"}"        â”‚
  â”‚                                              â”‚
  â”‚ http_request = format!(                     â”‚
  â”‚   "POST /input HTTP/1.1\r\n"                â”‚
  â”‚   "Host: localhost\r\n"                     â”‚
  â”‚   "Content-Type: application/json\r\n"     â”‚
  â”‚   "Content-Length: 20\r\n"                  â”‚
  â”‚   "Connection: close\r\n"                   â”‚
  â”‚   "\r\n"                                    â”‚
  â”‚   "{\"input\": \"hello\"}"                   â”‚
  â”‚ )                                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: Send via Unix socket (socket_http_post)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ stream = UnixStream::connect("/tmp/ai-       â”‚
  â”‚                     engine.sock").await       â”‚
  â”‚                                               â”‚
  â”‚ stream.write_all(http_request.as_bytes())    â”‚
  â”‚        .await                                â”‚
  â”‚                                               â”‚
  â”‚ // Request transmitted over Unix socket      â”‚
  â”‚ // Hypercorn receives and parses              â”‚
  â”‚ // Starlette route handler executes          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 4: Receive response
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ response_string = String::new()               â”‚
  â”‚ stream.read_to_string(&mut response_string)   â”‚
  â”‚       .await                                  â”‚
  â”‚                                               â”‚
  â”‚ // Raw HTTP response (with headers):          â”‚
  â”‚ // "HTTP/1.1 200 OK\r\n                      â”‚
  â”‚ //  Content-Type: application/json\r\n       â”‚
  â”‚ //  Content-Length: 95\r\n                   â”‚
  â”‚ //  \r\n                                      â”‚
  â”‚ //  {\"input\": \"hello\",                    â”‚
  â”‚ //   \"message\": \"You said: hello\",        â”‚
  â”‚ //   \"count\": 3,                            â”‚
  â”‚ //   \"timestamp\": 1234567890.5}"            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 5: Parse HTTP response and extract JSON
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Split response at "\r\n\r\n":                  â”‚
  â”‚   [0] = HTTP headers                           â”‚
  â”‚   [1] = JSON body                              â”‚
  â”‚                                                 â”‚
  â”‚ json_data = serde_json::from_str(body)         â”‚
  â”‚ // Parsed into serde_json::Value               â”‚
  â”‚ = {                                            â”‚
  â”‚     "input": "hello",                          â”‚
  â”‚     "message": "You said: hello",              â”‚
  â”‚     "count": 3,                                â”‚
  â”‚     "timestamp": 1234567890.5                  â”‚
  â”‚   }                                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 6: Emit event to frontend
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ app.emit("python_input",           â”‚
  â”‚          json_data.to_string())     â”‚
  â”‚                                     â”‚
  â”‚ // Serialized to JSON string:      â”‚
  â”‚ // "{\"input\":\"hello\",          â”‚
  â”‚ //   \"message\":\"You said:...\"}  â”‚
  â”‚                                     â”‚
  â”‚ â†’ Frontend receives event           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 7: Frontend processes event
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ listen("python_input", (event) => {    â”‚
  â”‚   const data = JSON.parse(event.payload)â”‚
  â”‚   setInputOutput(data)                  â”‚
  â”‚ })                                      â”‚
  â”‚                                         â”‚
  â”‚ // React re-renders InputCard component â”‚
  â”‚ // Display:                             â”‚
  â”‚ //   Your Input: hello                  â”‚
  â”‚ //   Echo: You said: hello              â”‚
  â”‚ //   Time: 14:07:30                     â”‚
  â”‚                                         â”‚
  â”‚ setInput("")  // Clear input field      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TOTAL TIME: ~50-100ms (dominated by Python processing)
```

---

## Stop/Shutdown Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   STOP_PYTHON_SCRIPT() & GRACEFUL SHUTDOWN         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRIGGER OPTIONS:
  1. User clicks "Stop Python Script" button (manual)
  2. Idle timeout reached after 5 minutes (automatic)
  3. App close (Tauri auto-cleanup)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO 1: MANUAL STOP (User clicks button)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: Frontend invokes command
  Frontend: invoke("stop_python_script")

STEP 2: Check if running
  proc_state = lock()
  if is_running == false:
    return Ok()  â† Already stopped

STEP 3: Send graceful stop signal
  socket_http_post("/stop", {})
  â”‚
  â”œâ”€ Construct: POST /stop HTTP/1.1\r\n...
  â”œâ”€ Connect to /tmp/ai-engine.sock
  â”œâ”€ Send request
  â”‚
  â””â”€â†’ Python receives and handles:
      @app.route('/stop', methods=['POST'])
      async def stop_handler(request):
        state.running = False
        os.kill(os.getpid(), signal.SIGTERM)
        return JSONResponse({"status": "stopping"})

STEP 4: Graceful wait period
  tokio::time::sleep(Duration::from_millis(500))
  // Give Python 500ms to shut down cleanly

STEP 5: Force terminate if needed
  if child.is_alive():
    child.kill()  // Force SIGKILL if timeout

STEP 6: Mark as stopped
  is_running = false
  child = None

STEP 7: Return to frontend
  Result: Ok(())


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO 2: IDLE TIMEOUT STOP (5 minutes)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Happens inside polling loop:

STEP 1: Polling thread detects timeout
  last_activity_elapsed = now() - last_activity
  if last_activity_elapsed > 300 seconds:
    println!("Idle timeout reached")

STEP 2: Send stop signal (same as manual)
  socket_http_post("/stop", {})
  â†’ Python graceful shutdown

STEP 3: Mark stopped and exit loop
  is_running = false
  break  // Exit polling loop forever
  
STEP 4: Frontend notification
  No automatic notification (polling stops)
  User would notice lack of status updates


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO 3: APP CLOSURE SHUTDOWN                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: User closes Tauri window
  Tauri lifecycle event: on_close()

STEP 2: Tauri cleanup handlers
  For each managed process:
    if process.is_running():
      process.terminate()  // SIGTERM
      await timeout(5 seconds)
      if still_alive():
        process.kill()  // SIGKILL

STEP 3: Process tree terminated
  All children cleaned up
  All sockets closed
  All file handles released
```

---

## Unix Socket Communication

### The Socket Layer (Rust)

```rust
// socket_http_get() - Send HTTP GET over Unix socket
async fn socket_http_get(socket_path: &str, endpoint: &str) 
  â†’ Result<serde_json::Value, String>

Step-by-step:
  1. Connect to Unix socket
     stream = UnixStream::connect("/tmp/ai-engine.sock").await
  
  2. Build HTTP GET request
     request = format!(
       "GET /status HTTP/1.1\r\n
        Host: localhost\r\n
        Connection: close\r\n\r\n"
     )
  
  3. Write to socket (async)
     stream.write_all(request.as_bytes()).await
  
  4. Read entire response (async)
     stream.read_to_string(&mut response).await
  
  5. Parse HTTP response
     Split at "\r\n\r\n" to separate headers from body
     Extract JSON from body portion
  
  6. Return Result<JSON>


// socket_http_post() - Send HTTP POST with JSON body
async fn socket_http_post(socket_path: &str, endpoint: &str, body: &Value)
  â†’ Result<serde_json::Value, String>

Similar flow, but:
  - Include "Content-Type: application/json"
  - Include "Content-Length: {}"
  - Append JSON body after headers
  - Handle empty response bodies (return {})
```

### Performance Characteristics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                   â”‚ Unix Socket   â”‚ TCP/IPv4 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Latency per request      â”‚ 0.05-0.1 ms   â”‚ 0.5-2 ms â”‚
â”‚ Context switches         â”‚ 2-3 per req   â”‚ 6-8      â”‚
â”‚ Memory overhead          â”‚ Minimal       â”‚ ~20 KB   â”‚
â”‚ Port conflicts           â”‚ No            â”‚ Yes      â”‚
â”‚ File permissions control â”‚ Yes (0o600)   â”‚ No       â”‚
â”‚ Network monitoring       â”‚ Hidden        â”‚ Visible  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Performance advantage: ~10-20x faster than TCP localhost
Overhead mostly from serialization, not transport
```

---

## Framework Comparison

### Why These Frameworks?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRAMEWORK SELECTION MATRIX                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Category     â”‚ Selected        â”‚ Async    â”‚ Use Case â”‚ Memory     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FRONTEND     â”‚ React + Vite    â”‚ âœ“        â”‚ Fast UI  â”‚ 50 MB      â”‚
â”‚              â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ Vue 3         â”‚ âœ“        â”‚ Similar  â”‚ 45 MB      â”‚
â”‚              â”‚ â€¢ Svelte        â”‚ âœ“        â”‚ Lighter  â”‚ 35 MB      â”‚
â”‚              â”‚ â€¢ Solid.js      â”‚ âœ“        â”‚ Minimal  â”‚ 25 MB      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DESKTOP      â”‚ Tauri           â”‚ âœ“        â”‚ Cross-OS â”‚ 80 MB      â”‚
â”‚ FRAMEWORK    â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ Electron      â”‚ âœ“        â”‚ Heavier  â”‚ 400+ MB    â”‚
â”‚              â”‚ â€¢ NW.js         â”‚ âœ“        â”‚ Legacy   â”‚ 350 MB     â”‚
â”‚              â”‚ â€¢ PyQt          â”‚ âœ—        â”‚ Python   â”‚ 200 MB     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PROCESS MGR  â”‚ Tauri Shell     â”‚ âœ“        â”‚ IPC+Cmd  â”‚ Built-in   â”‚
â”‚              â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ std::process  â”‚ âœ—        â”‚ Basic    â”‚ Built-in   â”‚
â”‚              â”‚ â€¢ subprocess    â”‚ âœ—        â”‚ Limited  â”‚ Limited    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ IPC          â”‚ Unix Socket     â”‚ âœ“        â”‚ Low latency â”‚ Kernel   â”‚
â”‚              â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ TCP localhost â”‚ âœ“        â”‚ Simpler  â”‚ More OH    â”‚
â”‚              â”‚ â€¢ Pipes         â”‚ âœ—        â”‚ Streamingâ”‚ Limited    â”‚
â”‚              â”‚ â€¢ Message Queue â”‚ âœ“        â”‚ Complex  â”‚ Heavy      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PYTHON ASGI  â”‚ Hypercorn       â”‚ âœ“        â”‚ UDS      â”‚ 40 MB      â”‚
â”‚ SERVER       â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ Uvicorn       â”‚ âœ“        â”‚ TCP only â”‚ 35 MB      â”‚
â”‚              â”‚ â€¢ Gunicorn      â”‚ âœ—        â”‚ Multi-procâ”‚ 50 MB     â”‚
â”‚              â”‚ â€¢ Daphne        â”‚ âœ“        â”‚ Django   â”‚ 45 MB      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WEB FRAME    â”‚ Starlette       â”‚ âœ“        â”‚ Lightweightâ”‚ 30 MB    â”‚
â”‚              â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ FastAPI       â”‚ âœ“        â”‚ Features â”‚ 35 MB      â”‚
â”‚              â”‚ â€¢ Django        â”‚ âœ—        â”‚ Heavy    â”‚ 80 MB      â”‚
â”‚              â”‚ â€¢ Flask         â”‚ âœ—        â”‚ Sync     â”‚ 25 MB      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BINARY BUILD â”‚ PyInstaller     â”‚ N/A      â”‚ Self-containedâ”‚ Var    â”‚
â”‚              â”‚ Alternative:    â”‚          â”‚          â”‚            â”‚
â”‚              â”‚ â€¢ py2exe        â”‚ N/A      â”‚ Windows  â”‚ Variable   â”‚
â”‚              â”‚ â€¢ cx_Freeze     â”‚ N/A      â”‚ Multi-OS â”‚ Variable   â”‚
â”‚              â”‚ â€¢ Nuitka        â”‚ N/A      â”‚ Compile  â”‚ Smaller    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Rationale

| Component | Choice | Why NOT Alternatives |
|-----------|--------|----------------------|
| **React** | Modern, hooks-based | Vue is similar; Svelte less ecosystem; Solid too niche |
| **Vite** | Lightning-fast build | Webpack too slow; Parcel unnecessary complexity |
| **Tauri** | Lightweight desktop | Electron 5x heavier memory; more secure than old PyQt |
| **Unix Socket** | True enterprise IPC | TCP adds 10-20ms latency; named pipes Windows-only |
| **Hypercorn** | Native UDS support | Uvicorn only does TCP; Gunicorn multiprocess overhead |
| **Starlette** | Minimal async framework | FastAPI adds OpenAPI overhead; Flask not async |
| **PyInstaller** | Simple self-contained | Nuitka requires compilation; cx_Freeze dated |

### Trade-offs

```
What we gain:
  âœ“ Memory efficiency (TensorFlow models stay resident)
  âœ“ Performance (10x faster IPC vs TCP localhost)
  âœ“ Enterprise security (Unix permissions + socket)
  âœ“ Cross-platform (macOS/Linux sidecar pattern works everywhere)
  âœ“ Type safety (Rust + TypeScript)

What we trade:
  âœ— Cannot easily debug with curl (socket not HTTP)
  âœ— Windows needs separate named pipe implementation
  âœ— Slightly more complex startup (socket readiness detection)
  âœ— Less ecosystem tooling for Unix sockets than TCP
```

---

## Performance Benchmarks

```
Operation Latencies (macOS, M1 Pro):
  â””â”€ Process spawn to socket ready: 150-200ms
  â””â”€ GET /status request: 1-2ms
  â””â”€ POST /input request: 2-5ms
  â””â”€ Parsing & event emission: 5-10ms
  â””â”€ Frontend render: 16-33ms (60 FPS)

Total user input â†’ display: ~30-60ms (imperceptible)

Memory:
  â””â”€ Frontend (React): 50 MB
  â””â”€ Tauri runtime: 80 MB
  â””â”€ Python binary: 150-200 MB (depends on PyInstaller size)
  â””â”€ TensorFlow model: 200-500 MB (on-demand loading)
  â””â”€ PyTorch model: 300-1000 MB (on-demand loading)

Idle state (Python stopped): 150 MB saved
```

---

## Deployment Checklist

- [ ] Build binary for target platforms: `bash python/build_binary.sh`
- [ ] Verify binary architecture: `file src-tauri/binaries/ai-engine-*`
- [ ] Test Unix socket creation: Check `/tmp/ai-engine.sock` exists
- [ ] Monitor idle timeout: Verify process stops after 5 min inactivity
- [ ] Test graceful shutdown: Kill process mid-request, verify cleanup
- [ ] Code signing (macOS): `codesign -s -` before distribution
- [ ] Notarization (macOS): Submit to Apple for verification
- [ ] Windows named pipe: Implement for Windows compatibility
- [ ] CI/CD pipeline: Automate binary builds on commit

---

## Future Improvements

1. **Cross-platform sockets**
   - Implement Windows named pipes (`\\.\pipe\ai-engine`)
   - Add platform detection in Rust

2. **Direct Python calling**
   - PyO3 bindings for hot functions
   - Avoid IPC overhead for small operations

3. **Binary distribution**
   - Code signing and notarization
   - Auto-update mechanism
   - Delta compression

4. **Monitoring & observability**
   - Metrics export (Prometheus format)
   - Structured logging
   - Performance profiling

5. **Advanced features**
   - Multiple Python sidecars
   - Load balancing across sockets
   - Hot-reload of Python code